# READINGS

Levy and Steinert's works revolve around the topic of ethics of machines and artificial intelligence, based on how we as humans perceive them. Levy takes on a more micro-approach to see how we as individuals perceive robots in relation to animals and other humans, and Steinert takes machine ethics in the macro-perspective of the workforce. Honestly, it was quite difficult thinking of more questions and topics for the readings, as it is very closely related to my presentation about preparing for a future with AI in the workforce, but they were interesting reads overall.

------------------------------------

## 1. Humans = Robots/AI
In Levy's work, he points out how those of devout religions will possess the same beliefs of human emergence similar to a robot: we are created by a higher power. If this is what determins a conscious individual, how different are we than the robots we create? The first humans were also “made” by a programmer (in this case God), according to religious individuals: does this mean we do not exhibit consciousness as we were made by someone else?

## 2. Comparing the Suffering of Robots to Minorities
Is our perception of humanness in robots determined solely by how we think about them? In Levy's work, this marginalization of AI at present is much like the marginalization of slaves in the past. Is it moral to compare the debate of right given to robots as the same as the right given to ethnic minorities? Are we merely overlooking the biological difference of human and computer? Does this desensitize the sufferings the marginalized had to go through in the past, as they exhibit real emotion rather than programmed emotion?

## 3. Selfish Use of Robots & Meta-Stance 
Do we want machines to make moral decisions? Seeing as we create robot nannies for neglectful parents, is the argument about giving robots rights only stirred by our selfish need to protect ourselves from their possible deviancy from what they were programmed to do? If this is so, then what is the point of giving them emotions in the first place? Though we have constant debates about if we should treat robots with the same respect as we do with humans, won’t this disregard the purpose of why we made them in the first place? Wouldn’t moral decisions and emotions get in the way?

## 4. Experience or Implementation
What would be the best way to implement emotions and ethics into a robot-through giving them already existing information or allow them to learn through their own experiences? In the case forming relationships with individuals, would it be considered ethical to program emotions into robots if it leads to better relationships with robots, such as a child loving a robot nanny more than their own parent?

## 5. Inequity Aversion in Robotics
Is the reason we are still having the ongoing debates of robot ethics stirred by the fact that we still consider them to be lower than human intelligence? There is a concept called "inequity aversion"--where, if someone is worse off than us, we want them to have more rights and be better off. However, once they become better than us, that is when we wish for their downfall. Once robots reach a point where their intelligence is beyond human capability, will we begin to retract our wishes for their equality of rights?
