# READINGS

Levy and Steinert's works revolve around the topic of ethics of machines and artificial intelligence, based on how we as humans perceive them. Levy takes on a more micro-approach to see how we as individuals perceive robots in relation to animals and other humans, and Steinert takes machine ethics in the macro-perspective of the workforce. Honestly, it was quite difficult thinking of more questions and topics for the readings, as it is very closely related to my presentation about preparing for a future with AI in the workforce, but they were interesting reads overall.

------------------------------------

## 1. Humans = Robots/AI
In Levy's work, he points out how those of devout religions will possess the same beliefs of human emergence similar to a robot: we are created by a higher power. If this is what determines a conscious individual, how different are we than the robots we create? The first humans were also “made” by a programmer (in this case God), according to religious individuals: does this mean we do not exhibit consciousness as we were made by someone else?

## 2. Comparing the Suffering of Robots to Minorities
Is our perception of humanness in robots determined solely by how we think about them? In Levy's work, this marginalization of AI at present is much like the marginalization of slaves in the past. Is it moral to compare the debate of right given to robots as the same as the right given to ethnic minorities? Are we merely overlooking the biological difference of human and computer? Does this desensitize the sufferings the marginalized had to go through in the past, as they exhibit real emotion rather than programmed emotion?

## 3. Inequity Aversion in Robotics
Is the reason we are still having the ongoing debates of robot ethics stirred by the fact that we still consider them to be lower than human intelligence? There is a concept called "inequity aversion"--where, if someone is worse off than us, we want them to have more rights and be better off. However, once they become better than us, that is when we wish for their downfall. Once robots reach a point where their intelligence is beyond human capability, will we begin to retract our wishes for their equality of rights?

## 4. Love & Friendships With Machines
What would be the best way to implement emotions and ethics into a robot: through giving them already existing information or allow them to learn through their own experiences? In the case of forming relationships with individuals, would it be considered ethical to program emotions into robots if it leads to better relationships with them, such as a child loving a robot nanny more than their own parent? Are we even ready for the conversation of falling in love with a robot?

## 5. Machine Ethics Now
What makes me so curious is why we as humans are already rushing to find the ethics behind machines who do not even possess the emotion and intellect of a human yet. As said by one of my friends when talking to him about this: "we are still light years away from this happening". This leads me to question: why now? We still haven't figured out all the problems of the human race, from racism, to mysogyny, to transphobia. Why is this such an important conversation to make *now*?

## 6. Selfish Use of Robots & Meta-Stance 
Seeing as we create machines to protect ourselves from human mistakes, is the argument about giving robots rights only stirred by our selfish need to protect ourselves from their possible deviancy from what they were programmed to do? If this is so, then what is the point of giving them emotions? Though we have constant debates about if we should treat robots with the same respect as we do with humans, won’t this disregard the purpose of why we made them in the first place? Wouldn’t moral decisions and emotions get in the way?
